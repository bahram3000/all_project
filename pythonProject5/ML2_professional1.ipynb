{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst radius  worst texture  \\\n0                   0.07871  ...        25.380          17.33   \n1                   0.05667  ...        24.990          23.41   \n2                   0.05999  ...        23.570          25.53   \n3                   0.09744  ...        14.910          26.50   \n4                   0.05883  ...        22.540          16.67   \n..                      ...  ...           ...            ...   \n564                 0.05623  ...        25.450          26.40   \n565                 0.05533  ...        23.690          38.25   \n566                 0.05648  ...        18.980          34.12   \n567                 0.07016  ...        25.740          39.42   \n568                 0.05884  ...         9.456          30.37   \n\n     worst perimeter  worst area  worst smoothness  worst compactness  \\\n0             184.60      2019.0           0.16220            0.66560   \n1             158.80      1956.0           0.12380            0.18660   \n2             152.50      1709.0           0.14440            0.42450   \n3              98.87       567.7           0.20980            0.86630   \n4             152.20      1575.0           0.13740            0.20500   \n..               ...         ...               ...                ...   \n564           166.10      2027.0           0.14100            0.21130   \n565           155.00      1731.0           0.11660            0.19220   \n566           126.70      1124.0           0.11390            0.30940   \n567           184.60      1821.0           0.16500            0.86810   \n568            59.16       268.6           0.08996            0.06444   \n\n     worst concavity  worst concave points  worst symmetry  \\\n0             0.7119                0.2654          0.4601   \n1             0.2416                0.1860          0.2750   \n2             0.4504                0.2430          0.3613   \n3             0.6869                0.2575          0.6638   \n4             0.4000                0.1625          0.2364   \n..               ...                   ...             ...   \n564           0.4107                0.2216          0.2060   \n565           0.3215                0.1628          0.2572   \n566           0.3403                0.1418          0.2218   \n567           0.9387                0.2650          0.4087   \n568           0.0000                0.0000          0.2871   \n\n     worst fractal dimension  \n0                    0.11890  \n1                    0.08902  \n2                    0.08758  \n3                    0.17300  \n4                    0.07678  \n..                       ...  \n564                  0.07115  \n565                  0.06637  \n566                  0.07820  \n567                  0.12400  \n568                  0.07039  \n\n[569 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brs = load_breast_cancer()\n",
    "brs_df = pd.DataFrame(brs.data, columns=brs.feature_names)\n",
    "brs_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brs_df['target'] = brs.target\n",
    "brs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['malignant', 'benign'], dtype='<U9')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brs.target_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_13948\\1563169771.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  brs_df['target_names'][brs_df.target == 0] = brs.target_names[0]\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_13948\\1563169771.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  brs_df['target_names'][brs_df.target == 1] = brs.target_names[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst perimeter  worst area  \\\n0                   0.07871  ...           184.60      2019.0   \n1                   0.05667  ...           158.80      1956.0   \n2                   0.05999  ...           152.50      1709.0   \n3                   0.09744  ...            98.87       567.7   \n4                   0.05883  ...           152.20      1575.0   \n..                      ...  ...              ...         ...   \n564                 0.05623  ...           166.10      2027.0   \n565                 0.05533  ...           155.00      1731.0   \n566                 0.05648  ...           126.70      1124.0   \n567                 0.07016  ...           184.60      1821.0   \n568                 0.05884  ...            59.16       268.6   \n\n     worst smoothness  worst compactness  worst concavity  \\\n0             0.16220            0.66560           0.7119   \n1             0.12380            0.18660           0.2416   \n2             0.14440            0.42450           0.4504   \n3             0.20980            0.86630           0.6869   \n4             0.13740            0.20500           0.4000   \n..                ...                ...              ...   \n564           0.14100            0.21130           0.4107   \n565           0.11660            0.19220           0.3215   \n566           0.11390            0.30940           0.3403   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     worst concave points  worst symmetry  worst fractal dimension  target  \\\n0                  0.2654          0.4601                  0.11890       0   \n1                  0.1860          0.2750                  0.08902       0   \n2                  0.2430          0.3613                  0.08758       0   \n3                  0.2575          0.6638                  0.17300       0   \n4                  0.1625          0.2364                  0.07678       0   \n..                    ...             ...                      ...     ...   \n564                0.2216          0.2060                  0.07115       0   \n565                0.1628          0.2572                  0.06637       0   \n566                0.1418          0.2218                  0.07820       0   \n567                0.2650          0.4087                  0.12400       0   \n568                0.0000          0.2871                  0.07039       1   \n\n     target_names  \n0       malignant  \n1       malignant  \n2       malignant  \n3       malignant  \n4       malignant  \n..            ...  \n564     malignant  \n565     malignant  \n566     malignant  \n567     malignant  \n568        benign  \n\n[569 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n      <th>target_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>0</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>1</td>\n      <td>benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brs_df['target_names'] = ''\n",
    "brs_df['target_names'][brs_df.target == 0] = brs.target_names[0]\n",
    "brs_df['target_names'][brs_df.target == 1] = brs.target_names[1]\n",
    "brs_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "x = brs.data\n",
    "y = brs.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n        1.189e-01],\n       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n        8.902e-02],\n       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n        8.758e-02],\n       ...,\n       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n        7.820e-02],\n       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n        1.240e-01],\n       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n        7.039e-02]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_encoder = LabelEncoder()\n",
    "lb_encoder.fit(brs_df.target_names)\n",
    "labels = lb_encoder.transform(brs_df.target_names)\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.9201802 ,  0.93964435,  1.91433302, ...,  1.45039486,\n         0.3636515 , -0.29189084],\n       [-0.42152734, -1.66379749, -0.4720893 , ..., -0.49867711,\n        -0.20507255,  0.56734933],\n       [ 0.11274574,  0.47564198,  0.20780662, ...,  2.35609179,\n         2.20754887,  2.31562375],\n       ...,\n       [-0.37321542,  0.02078252, -0.39846475, ..., -0.77828824,\n         0.66664694, -0.35254309],\n       [-0.70287327, -0.63522082, -0.72545644, ..., -0.6895421 ,\n        -1.92744768, -0.56110169],\n       [-0.37889917,  0.43907037, -0.40011   , ..., -0.89727494,\n        -0.67009764, -0.13121559]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "x_tr = sc.fit_transform(x_tr)\n",
    "x_ts = sc.transform(x_ts)\n",
    "x_tr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "NN = Sequential()\n",
    "NN.add(Dense(1, activation='sigmoid', input_shape=(30,)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "NN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10/10 [==============================] - 1s 2ms/step - loss: 1.7787 - accuracy: 0.1033\n",
      "Epoch 2/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.6634 - accuracy: 0.1033\n",
      "Epoch 3/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.5558 - accuracy: 0.1055\n",
      "Epoch 4/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.4537 - accuracy: 0.1099\n",
      "Epoch 5/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.3521 - accuracy: 0.1231\n",
      "Epoch 6/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.2587 - accuracy: 0.1473\n",
      "Epoch 7/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.1739 - accuracy: 0.1604\n",
      "Epoch 8/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0906 - accuracy: 0.1912\n",
      "Epoch 9/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.0139 - accuracy: 0.2352\n",
      "Epoch 10/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9435 - accuracy: 0.2703\n",
      "Epoch 11/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8797 - accuracy: 0.3341\n",
      "Epoch 12/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8229 - accuracy: 0.4132\n",
      "Epoch 13/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7700 - accuracy: 0.4637\n",
      "Epoch 14/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.7222 - accuracy: 0.5451\n",
      "Epoch 15/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 0.5890\n",
      "Epoch 16/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.6330\n",
      "Epoch 17/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6046 - accuracy: 0.6769\n",
      "Epoch 18/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.7275\n",
      "Epoch 19/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.7582\n",
      "Epoch 20/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7802\n",
      "Epoch 21/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5007 - accuracy: 0.7934\n",
      "Epoch 22/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.8154\n",
      "Epoch 23/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.8352\n",
      "Epoch 24/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.8527\n",
      "Epoch 25/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4288 - accuracy: 0.8593\n",
      "Epoch 26/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4146 - accuracy: 0.8703\n",
      "Epoch 27/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4013 - accuracy: 0.8747\n",
      "Epoch 28/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8901\n",
      "Epoch 29/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8989\n",
      "Epoch 30/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3648 - accuracy: 0.9077\n",
      "Epoch 31/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.9099\n",
      "Epoch 32/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.9143\n",
      "Epoch 33/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.9165\n",
      "Epoch 34/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.9187\n",
      "Epoch 35/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.9253\n",
      "Epoch 36/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.9275\n",
      "Epoch 37/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3045 - accuracy: 0.9275\n",
      "Epoch 38/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2979 - accuracy: 0.9297\n",
      "Epoch 39/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.9297\n",
      "Epoch 40/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.9297\n",
      "Epoch 41/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.9297\n",
      "Epoch 42/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.9319\n",
      "Epoch 43/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.9341\n",
      "Epoch 44/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2633 - accuracy: 0.9363\n",
      "Epoch 45/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2583 - accuracy: 0.9385\n",
      "Epoch 46/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.9407\n",
      "Epoch 47/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9407\n",
      "Epoch 48/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2446 - accuracy: 0.9429\n",
      "Epoch 49/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9429\n",
      "Epoch 50/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.9429\n",
      "Epoch 51/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.9473\n",
      "Epoch 52/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2291 - accuracy: 0.9516\n",
      "Epoch 53/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2257 - accuracy: 0.9516\n",
      "Epoch 54/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2223 - accuracy: 0.9516\n",
      "Epoch 55/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2190 - accuracy: 0.9516\n",
      "Epoch 56/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9516\n",
      "Epoch 57/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9538\n",
      "Epoch 58/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9538\n",
      "Epoch 59/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9538\n",
      "Epoch 60/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2041 - accuracy: 0.9538\n",
      "Epoch 61/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9538\n",
      "Epoch 62/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9538\n",
      "Epoch 63/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.9538\n",
      "Epoch 64/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1940 - accuracy: 0.9538\n",
      "Epoch 65/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9560\n",
      "Epoch 66/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9582\n",
      "Epoch 67/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9604\n",
      "Epoch 68/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9604\n",
      "Epoch 69/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 0.9604\n",
      "Epoch 70/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9604\n",
      "Epoch 71/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1792 - accuracy: 0.9604\n",
      "Epoch 72/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9604\n",
      "Epoch 73/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1754 - accuracy: 0.9604\n",
      "Epoch 74/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9604\n",
      "Epoch 75/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9604\n",
      "Epoch 76/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9604\n",
      "Epoch 77/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.9604\n",
      "Epoch 78/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.9604\n",
      "Epoch 79/150\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9648\n",
      "Epoch 80/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9648\n",
      "Epoch 81/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1625 - accuracy: 0.9648\n",
      "Epoch 82/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1611 - accuracy: 0.9648\n",
      "Epoch 83/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9648\n",
      "Epoch 84/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1584 - accuracy: 0.9648\n",
      "Epoch 85/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9648\n",
      "Epoch 86/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9648\n",
      "Epoch 87/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.9648\n",
      "Epoch 88/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9648\n",
      "Epoch 89/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1520 - accuracy: 0.9670\n",
      "Epoch 90/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9692\n",
      "Epoch 91/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1497 - accuracy: 0.9714\n",
      "Epoch 92/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9714\n",
      "Epoch 93/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9714\n",
      "Epoch 94/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9714\n",
      "Epoch 95/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1452 - accuracy: 0.9714\n",
      "Epoch 96/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1441 - accuracy: 0.9714\n",
      "Epoch 97/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9714\n",
      "Epoch 98/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1421 - accuracy: 0.9714\n",
      "Epoch 99/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1411 - accuracy: 0.9714\n",
      "Epoch 100/150\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.1401 - accuracy: 0.9736\n",
      "Epoch 101/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1392 - accuracy: 0.9736\n",
      "Epoch 102/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9736\n",
      "Epoch 103/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1373 - accuracy: 0.9736\n",
      "Epoch 104/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1364 - accuracy: 0.9736\n",
      "Epoch 105/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1355 - accuracy: 0.9736\n",
      "Epoch 106/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9736\n",
      "Epoch 107/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9736\n",
      "Epoch 108/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1330 - accuracy: 0.9736\n",
      "Epoch 109/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9736\n",
      "Epoch 110/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1314 - accuracy: 0.9780\n",
      "Epoch 111/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1307 - accuracy: 0.9780\n",
      "Epoch 112/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9802\n",
      "Epoch 113/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.9802\n",
      "Epoch 114/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9802\n",
      "Epoch 115/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9802\n",
      "Epoch 116/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.9802\n",
      "Epoch 117/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1263 - accuracy: 0.9802\n",
      "Epoch 118/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9802\n",
      "Epoch 119/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9802\n",
      "Epoch 120/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1242 - accuracy: 0.9802\n",
      "Epoch 121/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1236 - accuracy: 0.9802\n",
      "Epoch 122/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9802\n",
      "Epoch 123/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9802\n",
      "Epoch 124/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1217 - accuracy: 0.9802\n",
      "Epoch 125/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1212 - accuracy: 0.9802\n",
      "Epoch 126/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9802\n",
      "Epoch 127/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9802\n",
      "Epoch 128/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9802\n",
      "Epoch 129/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1188 - accuracy: 0.9802\n",
      "Epoch 130/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9802\n",
      "Epoch 131/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9802\n",
      "Epoch 132/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1171 - accuracy: 0.9802\n",
      "Epoch 133/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1166 - accuracy: 0.9802\n",
      "Epoch 134/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1160 - accuracy: 0.9802\n",
      "Epoch 135/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9802\n",
      "Epoch 136/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 0.9802\n",
      "Epoch 137/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9802\n",
      "Epoch 138/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1139 - accuracy: 0.9802\n",
      "Epoch 139/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.9802\n",
      "Epoch 140/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1129 - accuracy: 0.9802\n",
      "Epoch 141/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1124 - accuracy: 0.9802\n",
      "Epoch 142/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1119 - accuracy: 0.9802\n",
      "Epoch 143/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1114 - accuracy: 0.9802\n",
      "Epoch 144/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.9802\n",
      "Epoch 145/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9802\n",
      "Epoch 146/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1099 - accuracy: 0.9802\n",
      "Epoch 147/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1095 - accuracy: 0.9802\n",
      "Epoch 148/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9802\n",
      "Epoch 149/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.9802\n",
      "Epoch 150/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1081 - accuracy: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2669e4df160>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.fit(x_tr, y_tr, batch_size=50, epochs=150)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = NN.evaluate(x_ts, y_ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1090112030506134"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9736841917037964"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10/10 [==============================] - 1s 1ms/step - loss: 0.9604 - accuracy: 0.4857\n",
      "Epoch 2/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8407 - accuracy: 0.5451\n",
      "Epoch 3/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.7428 - accuracy: 0.6022\n",
      "Epoch 4/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6577 - accuracy: 0.6374\n",
      "Epoch 5/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6813\n",
      "Epoch 6/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.5251 - accuracy: 0.7143\n",
      "Epoch 7/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4813 - accuracy: 0.7429\n",
      "Epoch 8/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.4417 - accuracy: 0.7758\n",
      "Epoch 9/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8066\n",
      "Epoch 10/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8330\n",
      "Epoch 11/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8549\n",
      "Epoch 12/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8681\n",
      "Epoch 13/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3168 - accuracy: 0.8769\n",
      "Epoch 14/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2995 - accuracy: 0.8901\n",
      "Epoch 15/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2844 - accuracy: 0.8945\n",
      "Epoch 16/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.8989\n",
      "Epoch 17/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.9099\n",
      "Epoch 18/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.9209\n",
      "Epoch 19/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.9209\n",
      "Epoch 20/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.9319\n",
      "Epoch 21/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9319\n",
      "Epoch 22/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9319\n",
      "Epoch 23/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9385\n",
      "Epoch 24/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9407\n",
      "Epoch 25/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1899 - accuracy: 0.9407\n",
      "Epoch 26/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9407\n",
      "Epoch 27/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1784 - accuracy: 0.9407\n",
      "Epoch 28/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.9473\n",
      "Epoch 29/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1674 - accuracy: 0.9495\n",
      "Epoch 30/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1630 - accuracy: 0.9538\n",
      "Epoch 31/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1584 - accuracy: 0.9538\n",
      "Epoch 32/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9560\n",
      "Epoch 33/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9560\n",
      "Epoch 34/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9560\n",
      "Epoch 35/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.9604\n",
      "Epoch 36/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 0.9604\n",
      "Epoch 37/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9604\n",
      "Epoch 38/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.9626\n",
      "Epoch 39/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.9670\n",
      "Epoch 40/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9692\n",
      "Epoch 41/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9692\n",
      "Epoch 42/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9692\n",
      "Epoch 43/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9692\n",
      "Epoch 44/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9692\n",
      "Epoch 45/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9736\n",
      "Epoch 46/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.9736\n",
      "Epoch 47/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9736\n",
      "Epoch 48/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1094 - accuracy: 0.9780\n",
      "Epoch 49/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1074 - accuracy: 0.9780\n",
      "Epoch 50/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1056 - accuracy: 0.9802\n",
      "Epoch 51/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.9802\n",
      "Epoch 52/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9802\n",
      "Epoch 53/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.9802\n",
      "Epoch 54/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9802\n",
      "Epoch 55/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9802\n",
      "Epoch 56/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9802\n",
      "Epoch 57/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9802\n",
      "Epoch 58/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9802\n",
      "Epoch 59/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9824\n",
      "Epoch 60/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0895 - accuracy: 0.9824\n",
      "Epoch 61/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9824\n",
      "Epoch 62/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.9846\n",
      "Epoch 63/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9846\n",
      "Epoch 64/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9846\n",
      "Epoch 65/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9846\n",
      "Epoch 66/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.9846\n",
      "Epoch 67/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9846\n",
      "Epoch 68/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.9846\n",
      "Epoch 69/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9846\n",
      "Epoch 70/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9846\n",
      "Epoch 71/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9846\n",
      "Epoch 72/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9846\n",
      "Epoch 73/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9846\n",
      "Epoch 74/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9846\n",
      "Epoch 75/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9846\n",
      "Epoch 76/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9846\n",
      "Epoch 77/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9846\n",
      "Epoch 78/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9846\n",
      "Epoch 79/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9846\n",
      "Epoch 80/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9846\n",
      "Epoch 81/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0695 - accuracy: 0.9846\n",
      "Epoch 82/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9846\n",
      "Epoch 83/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.9846\n",
      "Epoch 84/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9846\n",
      "Epoch 85/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.9846\n",
      "Epoch 86/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9846\n",
      "Epoch 87/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9846\n",
      "Epoch 88/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.9846\n",
      "Epoch 89/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.9846\n",
      "Epoch 90/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9846\n",
      "Epoch 91/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9846\n",
      "Epoch 92/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9846\n",
      "Epoch 93/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9846\n",
      "Epoch 94/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9846\n",
      "Epoch 95/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9846\n",
      "Epoch 96/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9846\n",
      "Epoch 97/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9846\n",
      "Epoch 98/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9846\n",
      "Epoch 99/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9846\n",
      "Epoch 100/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9846\n",
      "Epoch 101/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9846\n",
      "Epoch 102/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9846\n",
      "Epoch 103/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9846\n",
      "Epoch 104/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9846\n",
      "Epoch 105/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9846\n",
      "Epoch 106/150\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9846\n",
      "Epoch 107/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9846\n",
      "Epoch 108/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9846\n",
      "Epoch 109/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9846\n",
      "Epoch 110/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9846\n",
      "Epoch 111/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9846\n",
      "Epoch 112/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9846\n",
      "Epoch 113/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9846\n",
      "Epoch 114/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9846\n",
      "Epoch 115/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9846\n",
      "Epoch 116/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9846\n",
      "Epoch 117/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 0.9868\n",
      "Epoch 118/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9868\n",
      "Epoch 119/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9868\n",
      "Epoch 120/150\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9868\n",
      "Epoch 121/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9868\n",
      "Epoch 122/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9868\n",
      "Epoch 123/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9868\n",
      "Epoch 124/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9868\n",
      "Epoch 125/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9868\n",
      "Epoch 126/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9868\n",
      "Epoch 127/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9868\n",
      "Epoch 128/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9868\n",
      "Epoch 129/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9868\n",
      "Epoch 130/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9846\n",
      "Epoch 131/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9846\n",
      "Epoch 132/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.9846\n",
      "Epoch 133/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9868\n",
      "Epoch 134/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9868\n",
      "Epoch 135/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.9868\n",
      "Epoch 136/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9868\n",
      "Epoch 137/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9890\n",
      "Epoch 138/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9890\n",
      "Epoch 139/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9890\n",
      "Epoch 140/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9890\n",
      "Epoch 141/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9890\n",
      "Epoch 142/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9890\n",
      "Epoch 143/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9890\n",
      "Epoch 144/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9890\n",
      "Epoch 145/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9890\n",
      "Epoch 146/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9868\n",
      "Epoch 147/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0464 - accuracy: 0.9868\n",
      "Epoch 148/150\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9868\n",
      "Epoch 149/150\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9868\n",
      "Epoch 150/150\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2669e8726b0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN2 = Sequential()\n",
    "NN2.add(Dense(5, activation='relu', input_shape=(30,)))\n",
    "NN2.add(Dense(1, activation='sigmoid'))\n",
    "NN2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "NN2.fit(x_tr, y_tr, batch_size=50, epochs=150)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 0s/step - loss: 0.0900 - accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "val_loss2, val_acc2 = NN2.evaluate(x_ts, y_ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0899614691734314"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9736841917037964"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#import tensorflow.python.keras.layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                  5.1               3.5                1.4               0.2\n1                  4.9               3.0                1.4               0.2\n2                  4.7               3.2                1.3               0.2\n3                  4.6               3.1                1.5               0.2\n4                  5.0               3.6                1.4               0.2\n..                 ...               ...                ...               ...\n145                6.7               3.0                5.2               2.3\n146                6.3               2.5                5.0               1.9\n147                6.5               3.0                5.2               2.0\n148                6.2               3.4                5.4               2.3\n149                5.9               3.0                5.1               1.8\n\n[150 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     target  \n0         0  \n1         0  \n2         0  \n3         0  \n4         0  \n..      ...  \n145       2  \n146       2  \n147       2  \n148       2  \n149       2  \n\n[150 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['target'] = iris.target\n",
    "iris_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ir = iris.data\n",
    "x_ir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ir = iris.target\n",
    "y_ir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "x_ = x_ir.copy()\n",
    "y_ = y_ir.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "for i in range(len(x_[0])):\n",
    "    m = np.min(x_[:, i])\n",
    "    x_[:, i] += -m\n",
    "    l = 0\n",
    "    for j in x_[:, i]:\n",
    "        l += j ** 2\n",
    "    l = l ** (1 / 2)\n",
    "    x_[:, i] /= l\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.03732251, 0.10714286, 0.00998345, 0.00611018],\n       [0.02799188, 0.07142857, 0.00998345, 0.00611018],\n       [0.01866126, 0.08571429, 0.00748759, 0.00611018],\n       [0.01399594, 0.07857143, 0.01247931, 0.00611018],\n       [0.0326572 , 0.11428571, 0.00998345, 0.00611018],\n       [0.05131845, 0.13571429, 0.01747103, 0.01833055],\n       [0.01399594, 0.1       , 0.00998345, 0.01222036],\n       [0.0326572 , 0.1       , 0.01247931, 0.00611018],\n       [0.00466531, 0.06428571, 0.00998345, 0.00611018],\n       [0.02799188, 0.07857143, 0.01247931, 0.        ],\n       [0.05131845, 0.12142857, 0.01247931, 0.00611018],\n       [0.02332657, 0.1       , 0.01497517, 0.00611018],\n       [0.02332657, 0.07142857, 0.00998345, 0.        ],\n       [0.        , 0.07142857, 0.00249586, 0.        ],\n       [0.06997971, 0.14285714, 0.00499172, 0.00611018],\n       [0.06531439, 0.17142857, 0.01247931, 0.01833055],\n       [0.05131845, 0.13571429, 0.00748759, 0.01833055],\n       [0.03732251, 0.10714286, 0.00998345, 0.01222036],\n       [0.06531439, 0.12857143, 0.01747103, 0.01222036],\n       [0.03732251, 0.12857143, 0.01247931, 0.01222036],\n       [0.05131845, 0.1       , 0.01747103, 0.00611018],\n       [0.03732251, 0.12142857, 0.01247931, 0.01833055],\n       [0.01399594, 0.11428571, 0.        , 0.00611018],\n       [0.03732251, 0.09285714, 0.01747103, 0.02444073],\n       [0.02332657, 0.1       , 0.02246276, 0.00611018],\n       [0.0326572 , 0.07142857, 0.01497517, 0.00611018],\n       [0.0326572 , 0.1       , 0.01497517, 0.01833055],\n       [0.04198783, 0.10714286, 0.01247931, 0.00611018],\n       [0.04198783, 0.1       , 0.00998345, 0.00611018],\n       [0.01866126, 0.08571429, 0.01497517, 0.00611018],\n       [0.02332657, 0.07857143, 0.01497517, 0.00611018],\n       [0.05131845, 0.1       , 0.01247931, 0.01833055],\n       [0.04198783, 0.15      , 0.01247931, 0.        ],\n       [0.05598377, 0.15714286, 0.00998345, 0.00611018],\n       [0.02799188, 0.07857143, 0.01247931, 0.00611018],\n       [0.0326572 , 0.08571429, 0.00499172, 0.00611018],\n       [0.05598377, 0.10714286, 0.00748759, 0.00611018],\n       [0.02799188, 0.11428571, 0.00998345, 0.        ],\n       [0.00466531, 0.07142857, 0.00748759, 0.00611018],\n       [0.03732251, 0.1       , 0.01247931, 0.00611018],\n       [0.0326572 , 0.10714286, 0.00748759, 0.01222036],\n       [0.00933063, 0.02142857, 0.00748759, 0.01222036],\n       [0.00466531, 0.08571429, 0.00748759, 0.00611018],\n       [0.0326572 , 0.10714286, 0.01497517, 0.03055091],\n       [0.03732251, 0.12857143, 0.02246276, 0.01833055],\n       [0.02332657, 0.07142857, 0.00998345, 0.01222036],\n       [0.03732251, 0.12857143, 0.01497517, 0.00611018],\n       [0.01399594, 0.08571429, 0.00998345, 0.00611018],\n       [0.04665314, 0.12142857, 0.01247931, 0.00611018],\n       [0.0326572 , 0.09285714, 0.00998345, 0.00611018],\n       [0.12596348, 0.08571429, 0.09234689, 0.07943237],\n       [0.09797159, 0.08571429, 0.08735517, 0.08554255],\n       [0.12129816, 0.07857143, 0.09733861, 0.08554255],\n       [0.05598377, 0.02142857, 0.07487586, 0.07332219],\n       [0.10263691, 0.05714286, 0.08985103, 0.08554255],\n       [0.06531439, 0.05714286, 0.08735517, 0.07332219],\n       [0.09330628, 0.09285714, 0.09234689, 0.09165274],\n       [0.02799188, 0.02857143, 0.05740482, 0.05499164],\n       [0.10730222, 0.06428571, 0.08985103, 0.07332219],\n       [0.04198783, 0.05      , 0.07237999, 0.07943237],\n       [0.0326572 , 0.        , 0.06239655, 0.05499164],\n       [0.07464502, 0.07142857, 0.07986758, 0.08554255],\n       [0.07931034, 0.01428571, 0.07487586, 0.05499164],\n       [0.08397565, 0.06428571, 0.09234689, 0.07943237],\n       [0.06064908, 0.06428571, 0.06489241, 0.07332219],\n       [0.11196753, 0.07857143, 0.0848593 , 0.07943237],\n       [0.06064908, 0.07142857, 0.08735517, 0.08554255],\n       [0.06997971, 0.05      , 0.07737172, 0.05499164],\n       [0.08864096, 0.01428571, 0.08735517, 0.08554255],\n       [0.06064908, 0.03571429, 0.07237999, 0.06110182],\n       [0.07464502, 0.08571429, 0.09484275, 0.1038731 ],\n       [0.08397565, 0.05714286, 0.07487586, 0.07332219],\n       [0.09330628, 0.03571429, 0.09733861, 0.08554255],\n       [0.08397565, 0.05714286, 0.09234689, 0.06721201],\n       [0.09797159, 0.06428571, 0.08236344, 0.07332219],\n       [0.10730222, 0.07142857, 0.0848593 , 0.07943237],\n       [0.11663285, 0.05714286, 0.09484275, 0.07943237],\n       [0.11196753, 0.07142857, 0.09983447, 0.09776292],\n       [0.07931034, 0.06428571, 0.08735517, 0.08554255],\n       [0.06531439, 0.04285714, 0.06239655, 0.05499164],\n       [0.05598377, 0.02857143, 0.06988413, 0.06110182],\n       [0.05598377, 0.02857143, 0.06738827, 0.05499164],\n       [0.06997971, 0.05      , 0.07237999, 0.06721201],\n       [0.07931034, 0.05      , 0.10233034, 0.09165274],\n       [0.05131845, 0.07142857, 0.08735517, 0.08554255],\n       [0.07931034, 0.1       , 0.08735517, 0.09165274],\n       [0.11196753, 0.07857143, 0.09234689, 0.08554255],\n       [0.09330628, 0.02142857, 0.0848593 , 0.07332219],\n       [0.06064908, 0.07142857, 0.07737172, 0.07332219],\n       [0.05598377, 0.03571429, 0.07487586, 0.07332219],\n       [0.05598377, 0.04285714, 0.0848593 , 0.06721201],\n       [0.08397565, 0.07142857, 0.08985103, 0.07943237],\n       [0.06997971, 0.04285714, 0.07487586, 0.06721201],\n       [0.0326572 , 0.02142857, 0.05740482, 0.05499164],\n       [0.06064908, 0.05      , 0.07986758, 0.07332219],\n       [0.06531439, 0.07142857, 0.07986758, 0.06721201],\n       [0.06531439, 0.06428571, 0.07986758, 0.07332219],\n       [0.08864096, 0.06428571, 0.08236344, 0.07332219],\n       [0.03732251, 0.03571429, 0.04991724, 0.06110182],\n       [0.06531439, 0.05714286, 0.07737172, 0.07332219],\n       [0.09330628, 0.09285714, 0.12479309, 0.14664438],\n       [0.06997971, 0.05      , 0.10233034, 0.10998328],\n       [0.13062879, 0.07142857, 0.12229723, 0.12220365],\n       [0.09330628, 0.06428571, 0.11480965, 0.1038731 ],\n       [0.10263691, 0.07142857, 0.11980137, 0.12831383],\n       [0.15395536, 0.07142857, 0.13976826, 0.12220365],\n       [0.02799188, 0.03571429, 0.08735517, 0.09776292],\n       [0.13995942, 0.06428571, 0.13228068, 0.1038731 ],\n       [0.11196753, 0.03571429, 0.11980137, 0.1038731 ],\n       [0.1352941 , 0.11428571, 0.12728895, 0.14664438],\n       [0.10263691, 0.08571429, 0.10233034, 0.11609347],\n       [0.09797159, 0.05      , 0.10732206, 0.10998328],\n       [0.11663285, 0.07142857, 0.11231378, 0.12220365],\n       [0.06531439, 0.03571429, 0.09983447, 0.11609347],\n       [0.06997971, 0.05714286, 0.10233034, 0.1405342 ],\n       [0.09797159, 0.08571429, 0.10732206, 0.13442401],\n       [0.10263691, 0.07142857, 0.11231378, 0.1038731 ],\n       [0.15862067, 0.12857143, 0.14226413, 0.12831383],\n       [0.15862067, 0.04285714, 0.14725585, 0.13442401],\n       [0.07931034, 0.01428571, 0.09983447, 0.08554255],\n       [0.12129816, 0.08571429, 0.11730551, 0.13442401],\n       [0.06064908, 0.05714286, 0.09733861, 0.11609347],\n       [0.15862067, 0.05714286, 0.14226413, 0.11609347],\n       [0.09330628, 0.05      , 0.09733861, 0.1038731 ],\n       [0.11196753, 0.09285714, 0.11730551, 0.12220365],\n       [0.1352941 , 0.08571429, 0.12479309, 0.1038731 ],\n       [0.08864096, 0.05714286, 0.09484275, 0.1038731 ],\n       [0.08397565, 0.07142857, 0.09733861, 0.1038731 ],\n       [0.09797159, 0.05714286, 0.11480965, 0.12220365],\n       [0.1352941 , 0.07142857, 0.11980137, 0.09165274],\n       [0.14462473, 0.05714286, 0.12728895, 0.10998328],\n       [0.1679513 , 0.12857143, 0.13477654, 0.11609347],\n       [0.09797159, 0.05714286, 0.11480965, 0.12831383],\n       [0.09330628, 0.05714286, 0.10233034, 0.08554255],\n       [0.08397565, 0.04285714, 0.11480965, 0.07943237],\n       [0.15862067, 0.07142857, 0.12728895, 0.13442401],\n       [0.09330628, 0.1       , 0.11480965, 0.1405342 ],\n       [0.09797159, 0.07857143, 0.11231378, 0.1038731 ],\n       [0.07931034, 0.07142857, 0.09484275, 0.1038731 ],\n       [0.12129816, 0.07857143, 0.10981792, 0.12220365],\n       [0.11196753, 0.07857143, 0.11480965, 0.1405342 ],\n       [0.12129816, 0.07857143, 0.10233034, 0.13442401],\n       [0.06997971, 0.05      , 0.10233034, 0.10998328],\n       [0.11663285, 0.08571429, 0.12229723, 0.13442401],\n       [0.11196753, 0.09285714, 0.11730551, 0.14664438],\n       [0.11196753, 0.07142857, 0.1048262 , 0.13442401],\n       [0.09330628, 0.03571429, 0.09983447, 0.10998328],\n       [0.10263691, 0.07142857, 0.1048262 , 0.11609347],\n       [0.08864096, 0.1       , 0.10981792, 0.13442401],\n       [0.07464502, 0.07142857, 0.10233034, 0.1038731 ]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "x_tr_ir, x_ts_ir, y_tr_ir, y_ts_ir = train_test_split(x_, y_, test_size=0.2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "NN_multy = Sequential()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "NN_multy.add(Dense(15, activation='sigmoid',input_dim=4))\n",
    "NN_multy.add(Dense(15, activation='relu', input_dim=4))\n",
    "NN_multy.add(Dense(3,activation='softmax'))\n",
    "NN_multy.add(Dense(1, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.03732251, 0.10714286, 0.00998345, 0.00611018],\n       [0.08397565, 0.05714286, 0.07487586, 0.07332219],\n       [0.06997971, 0.04285714, 0.07487586, 0.06721201],\n       [0.03732251, 0.10714286, 0.00998345, 0.01222036],\n       [0.0326572 , 0.07142857, 0.01497517, 0.00611018],\n       [0.15862067, 0.12857143, 0.14226413, 0.12831383],\n       [0.06064908, 0.07142857, 0.08735517, 0.08554255],\n       [0.02332657, 0.1       , 0.01497517, 0.00611018],\n       [0.07931034, 0.01428571, 0.07487586, 0.05499164],\n       [0.09330628, 0.1       , 0.11480965, 0.1405342 ],\n       [0.08864096, 0.06428571, 0.08236344, 0.07332219],\n       [0.08864096, 0.1       , 0.10981792, 0.13442401],\n       [0.0326572 , 0.        , 0.06239655, 0.05499164],\n       [0.0326572 , 0.08571429, 0.00499172, 0.00611018],\n       [0.06997971, 0.05      , 0.07237999, 0.06721201],\n       [0.03732251, 0.12857143, 0.01247931, 0.01222036],\n       [0.06531439, 0.07142857, 0.07986758, 0.06721201],\n       [0.10263691, 0.08571429, 0.10233034, 0.11609347],\n       [0.0326572 , 0.10714286, 0.00748759, 0.01222036],\n       [0.12129816, 0.07857143, 0.09733861, 0.08554255],\n       [0.0326572 , 0.1       , 0.01247931, 0.00611018],\n       [0.02799188, 0.07857143, 0.01247931, 0.        ],\n       [0.08397565, 0.05714286, 0.09234689, 0.06721201],\n       [0.11196753, 0.09285714, 0.11730551, 0.14664438],\n       [0.10263691, 0.07142857, 0.11231378, 0.1038731 ],\n       [0.09797159, 0.08571429, 0.10732206, 0.13442401],\n       [0.02332657, 0.07142857, 0.00998345, 0.01222036],\n       [0.12129816, 0.07857143, 0.10981792, 0.12220365],\n       [0.11663285, 0.08571429, 0.12229723, 0.13442401],\n       [0.06531439, 0.04285714, 0.06239655, 0.05499164],\n       [0.05131845, 0.1       , 0.01247931, 0.01833055],\n       [0.11196753, 0.07857143, 0.09234689, 0.08554255],\n       [0.15862067, 0.04285714, 0.14725585, 0.13442401],\n       [0.10263691, 0.07142857, 0.1048262 , 0.11609347],\n       [0.07931034, 0.05      , 0.10233034, 0.09165274],\n       [0.05598377, 0.02142857, 0.07487586, 0.07332219],\n       [0.07464502, 0.08571429, 0.09484275, 0.1038731 ],\n       [0.0326572 , 0.10714286, 0.01497517, 0.03055091],\n       [0.04665314, 0.12142857, 0.01247931, 0.00611018],\n       [0.06064908, 0.06428571, 0.06489241, 0.07332219],\n       [0.06531439, 0.05714286, 0.08735517, 0.07332219],\n       [0.01399594, 0.07857143, 0.01247931, 0.00611018],\n       [0.00466531, 0.08571429, 0.00748759, 0.00611018],\n       [0.05598377, 0.03571429, 0.07487586, 0.07332219],\n       [0.11196753, 0.07857143, 0.11480965, 0.1405342 ],\n       [0.01866126, 0.08571429, 0.01497517, 0.00611018],\n       [0.05131845, 0.1       , 0.01747103, 0.00611018],\n       [0.05598377, 0.15714286, 0.00998345, 0.00611018],\n       [0.15862067, 0.07142857, 0.12728895, 0.13442401],\n       [0.06064908, 0.05      , 0.07986758, 0.07332219],\n       [0.08397565, 0.07142857, 0.09733861, 0.1038731 ],\n       [0.06531439, 0.06428571, 0.07986758, 0.07332219],\n       [0.11196753, 0.07142857, 0.1048262 , 0.13442401],\n       [0.1352941 , 0.07142857, 0.11980137, 0.09165274],\n       [0.02799188, 0.11428571, 0.00998345, 0.        ],\n       [0.04198783, 0.05      , 0.07237999, 0.07943237],\n       [0.08397565, 0.07142857, 0.08985103, 0.07943237],\n       [0.11663285, 0.07142857, 0.11231378, 0.12220365],\n       [0.03732251, 0.12857143, 0.02246276, 0.01833055],\n       [0.03732251, 0.1       , 0.01247931, 0.00611018],\n       [0.01399594, 0.11428571, 0.        , 0.00611018],\n       [0.06997971, 0.05      , 0.07737172, 0.05499164],\n       [0.04198783, 0.15      , 0.01247931, 0.        ],\n       [0.06064908, 0.07142857, 0.07737172, 0.07332219],\n       [0.03732251, 0.12142857, 0.01247931, 0.01833055],\n       [0.05598377, 0.04285714, 0.0848593 , 0.06721201],\n       [0.11196753, 0.07142857, 0.09983447, 0.09776292],\n       [0.10730222, 0.07142857, 0.0848593 , 0.07943237],\n       [0.09330628, 0.03571429, 0.09733861, 0.08554255],\n       [0.03732251, 0.12857143, 0.01497517, 0.00611018],\n       [0.10263691, 0.07142857, 0.11980137, 0.12831383],\n       [0.11196753, 0.03571429, 0.11980137, 0.1038731 ],\n       [0.05598377, 0.10714286, 0.00748759, 0.00611018],\n       [0.05131845, 0.13571429, 0.01747103, 0.01833055],\n       [0.02799188, 0.03571429, 0.08735517, 0.09776292],\n       [0.00466531, 0.06428571, 0.00998345, 0.00611018],\n       [0.07464502, 0.07142857, 0.10233034, 0.1038731 ],\n       [0.09330628, 0.06428571, 0.11480965, 0.1038731 ],\n       [0.15395536, 0.07142857, 0.13976826, 0.12220365],\n       [0.02332657, 0.07142857, 0.00998345, 0.        ],\n       [0.07931034, 0.06428571, 0.08735517, 0.08554255],\n       [0.00933063, 0.02142857, 0.00748759, 0.01222036],\n       [0.15862067, 0.05714286, 0.14226413, 0.11609347],\n       [0.09330628, 0.09285714, 0.12479309, 0.14664438],\n       [0.07931034, 0.01428571, 0.09983447, 0.08554255],\n       [0.11196753, 0.07857143, 0.0848593 , 0.07943237],\n       [0.1352941 , 0.11428571, 0.12728895, 0.14664438],\n       [0.11196753, 0.09285714, 0.11730551, 0.12220365],\n       [0.01399594, 0.08571429, 0.00998345, 0.00611018],\n       [0.09797159, 0.08571429, 0.08735517, 0.08554255],\n       [0.0326572 , 0.02142857, 0.05740482, 0.05499164],\n       [0.0326572 , 0.1       , 0.01497517, 0.01833055],\n       [0.09797159, 0.05714286, 0.11480965, 0.12831383],\n       [0.07931034, 0.07142857, 0.09484275, 0.1038731 ],\n       [0.01399594, 0.1       , 0.00998345, 0.01222036],\n       [0.08864096, 0.01428571, 0.08735517, 0.08554255],\n       [0.01866126, 0.08571429, 0.00748759, 0.00611018],\n       [0.06531439, 0.03571429, 0.09983447, 0.11609347],\n       [0.05598377, 0.02857143, 0.06988413, 0.06110182],\n       [0.07464502, 0.07142857, 0.07986758, 0.08554255],\n       [0.05131845, 0.12142857, 0.01247931, 0.00611018],\n       [0.06997971, 0.05      , 0.10233034, 0.10998328],\n       [0.00466531, 0.07142857, 0.00748759, 0.00611018],\n       [0.07931034, 0.1       , 0.08735517, 0.09165274],\n       [0.09797159, 0.05714286, 0.11480965, 0.12220365],\n       [0.06531439, 0.12857143, 0.01747103, 0.01222036],\n       [0.06997971, 0.14285714, 0.00499172, 0.00611018],\n       [0.11663285, 0.05714286, 0.09484275, 0.07943237],\n       [0.05598377, 0.02857143, 0.06738827, 0.05499164],\n       [0.12596348, 0.08571429, 0.09234689, 0.07943237],\n       [0.06997971, 0.05      , 0.10233034, 0.10998328],\n       [0.09330628, 0.05714286, 0.10233034, 0.08554255],\n       [0.06064908, 0.05714286, 0.09733861, 0.11609347],\n       [0.12129816, 0.07857143, 0.10233034, 0.13442401],\n       [0.09330628, 0.09285714, 0.09234689, 0.09165274],\n       [0.12129816, 0.08571429, 0.11730551, 0.13442401],\n       [0.02799188, 0.07142857, 0.00998345, 0.00611018],\n       [0.14462473, 0.05714286, 0.12728895, 0.10998328],\n       [0.08397565, 0.04285714, 0.11480965, 0.07943237],\n       [0.10263691, 0.05714286, 0.08985103, 0.08554255]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_ir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 1, 0, 0, 2, 1, 0, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0,\n       1, 2, 2, 2, 0, 2, 2, 1, 0, 1, 2, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n       2, 0, 0, 0, 2, 1, 2, 1, 2, 2, 0, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0, 1,\n       1, 1, 1, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 1, 0, 2, 2, 2, 1, 2, 2,\n       0, 1, 1, 0, 2, 2, 0, 1, 0, 2, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 1, 1,\n       2, 2, 2, 2, 1, 2, 0, 2, 2, 1])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_ir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 3s 2ms/step - loss: 0.6287 - accuracy: 0.3417\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.3417\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5766 - accuracy: 0.3417\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5538 - accuracy: 0.3417\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.3417\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.3417\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4967 - accuracy: 0.3417\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4797 - accuracy: 0.3417\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.3417\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4501 - accuracy: 0.3417\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4340 - accuracy: 0.3417\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.3417\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4100 - accuracy: 0.3417\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.3417\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3889 - accuracy: 0.3417\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.3417\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.3417\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.3417\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.3417\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.3417\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3419 - accuracy: 0.3417\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.3417\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.3417\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.3417\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.3417\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.3417\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.3417\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3045 - accuracy: 0.3417\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.3417\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.3417\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.3417\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.3417\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.3417\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2805 - accuracy: 0.3417\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.3417\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2736 - accuracy: 0.3417\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2698 - accuracy: 0.3417\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.3417\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.3417\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2602 - accuracy: 0.3417\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2573 - accuracy: 0.3417\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2538 - accuracy: 0.3417\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2516 - accuracy: 0.3417\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2478 - accuracy: 0.3417\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2453 - accuracy: 0.3417\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.3417\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2393 - accuracy: 0.3417\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.3417\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2345 - accuracy: 0.3417\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2319 - accuracy: 0.3417\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.3417\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2266 - accuracy: 0.3417\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2246 - accuracy: 0.3417\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2217 - accuracy: 0.3417\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2196 - accuracy: 0.3417\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2170 - accuracy: 0.3417\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2147 - accuracy: 0.3417\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.3417\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.3417\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2077 - accuracy: 0.3417\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2059 - accuracy: 0.3417\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.3417\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.3417\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.3417\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.3417\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1954 - accuracy: 0.3417\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1937 - accuracy: 0.3417\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1915 - accuracy: 0.3417\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1897 - accuracy: 0.3417\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.3417\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1858 - accuracy: 0.3417\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.3417\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1821 - accuracy: 0.3417\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.3417\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.3417\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1770 - accuracy: 0.3417\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1753 - accuracy: 0.3417\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1734 - accuracy: 0.3417\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.3417\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 0.3417\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1685 - accuracy: 0.3417\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.3417\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1652 - accuracy: 0.3417\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.3417\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.3417\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1608 - accuracy: 0.3417\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.3417\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.3417\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1564 - accuracy: 0.3417\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.3417\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.3417\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.3417\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.3417\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1491 - accuracy: 0.3417\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.3417\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.3417\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1450 - accuracy: 0.3417\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.3417\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.3417\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.3417\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x266a33c74c0>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_multy.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "NN_multy.fit(x_tr_ir, y_tr_ir, batch_size=15, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 223ms/step - loss: 0.2146 - accuracy: 0.3000\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.21462233364582062, 0.30000001192092896]"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_multy.evaluate(x_ts_ir,y_ts_ir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "NN_multy2=Sequential()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "sc2=StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "x2tr,x2ts,y2tr,y2ts=train_test_split(x_ir,y_ir,test_size=0.2,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.23593016, -0.12317529,  0.6500984 ,  0.84372816],\n       [ 1.12066825,  0.09743717,  1.11307649,  1.6483431 ],\n       [-0.39602562,  0.97988703, -1.37543074, -1.30191167],\n       [ 1.12066825,  0.5386621 ,  1.17094875,  1.24603563],\n       [-0.90159025,  0.5386621 , -1.1439417 , -0.8996042 ],\n       [-0.26963447, -0.78501269,  0.30286483,  0.17321572],\n       [-1.78632834,  0.31804964, -1.37543074, -1.30191167],\n       [-1.28076372, -0.12317529, -1.31755848, -1.16780918],\n       [ 0.23593016, -0.78501269,  0.82371518,  0.57552318],\n       [ 0.86788594, -0.12317529,  0.88158744,  1.11193314],\n       [-1.02798141, -0.12317529, -1.20181396, -1.30191167],\n       [ 0.48871247, -1.88807501,  0.47648162,  0.44142069],\n       [ 2.38457982,  1.64172442,  1.74967136,  1.38013812],\n       [ 1.12066825,  0.5386621 ,  1.17094875,  1.78244559],\n       [ 0.61510363, -0.34378776,  1.11307649,  0.84372816],\n       [-1.02798141,  1.20049949, -1.31755848, -1.30191167],\n       [-0.01685215,  2.08294935, -1.433303  , -1.30191167],\n       [ 0.74149478, -0.56440022,  1.11307649,  1.24603563],\n       [ 0.109539  , -0.12317529,  0.82371518,  0.84372816],\n       [-0.90159025,  1.42111196, -1.25968622, -1.03370669],\n       [-0.90159025,  0.97988703, -1.31755848, -1.16780918],\n       [-1.15437256, -1.22623762,  0.47648162,  0.70962567],\n       [-0.26963447, -1.22623762,  0.12924805, -0.09498926],\n       [-1.02798141,  0.75927457, -1.20181396, -1.03370669],\n       [-0.39602562, -1.44685008,  0.07137579, -0.09498926],\n       [ 0.86788594, -0.56440022,  0.53435388,  0.44142069],\n       [ 1.37345057,  0.09743717,  0.99733197,  1.24603563],\n       [-0.01685215, -0.56440022,  0.82371518,  1.6483431 ],\n       [ 1.12066825,  0.09743717,  0.41860936,  0.30731821],\n       [ 0.61510363, -1.66746254,  0.41860936,  0.17321572],\n       [ 0.48871247, -0.56440022,  0.6500984 ,  0.84372816],\n       [-1.02798141,  0.97988703, -1.20181396, -0.76550171],\n       [-0.39602562, -1.22623762,  0.18712031,  0.17321572],\n       [ 0.109539  , -0.12317529,  0.30286483,  0.44142069],\n       [ 1.12066825,  0.09743717,  0.59222614,  0.44142069],\n       [-0.01685215, -0.78501269,  0.24499257, -0.22909175],\n       [ 0.9942771 , -0.12317529,  0.41860936,  0.30731821],\n       [-0.39602562, -1.66746254,  0.18712031,  0.17321572],\n       [-0.77519909, -0.78501269,  0.12924805,  0.30731821],\n       [-0.52241678,  1.42111196, -1.25968622, -1.30191167],\n       [ 0.109539  ,  0.31804964,  0.6500984 ,  0.84372816],\n       [-0.77519909,  0.75927457, -1.31755848, -1.30191167],\n       [-1.15437256,  0.09743717, -1.25968622, -1.43601416],\n       [ 0.61510363, -1.22623762,  0.76584292,  0.97783065],\n       [-1.02798141,  0.31804964, -1.433303  , -1.30191167],\n       [-0.39602562, -1.44685008,  0.01350353, -0.22909175],\n       [ 0.23593016,  0.75927457,  0.47648162,  0.57552318],\n       [-0.90159025,  1.64172442, -1.02819717, -1.03370669],\n       [ 0.61510363,  0.75927457,  1.11307649,  1.6483431 ],\n       [-0.14324331,  2.96539921, -1.25968622, -1.03370669],\n       [ 1.12066825, -0.12317529,  0.76584292,  0.70962567],\n       [ 1.12066825, -1.22623762,  1.22882101,  0.84372816],\n       [ 0.74149478,  0.09743717,  1.05520423,  0.84372816],\n       [ 1.37345057,  0.09743717,  0.82371518,  1.51424061],\n       [-0.39602562,  2.52417428, -1.31755848, -1.30191167],\n       [-0.26963447, -0.12317529,  0.47648162,  0.44142069],\n       [ 0.23593016, -0.34378776,  0.47648162,  0.44142069],\n       [-1.15437256, -0.12317529, -1.31755848, -1.30191167],\n       [-1.02798141,  0.97988703, -1.37543074, -1.16780918],\n       [-1.53354603,  1.20049949, -1.54904752, -1.30191167],\n       [-0.64880794,  1.42111196, -1.25968622, -1.30191167],\n       [ 0.23593016, -1.88807501,  0.18712031, -0.22909175],\n       [-1.02798141, -2.32929994, -0.10224099, -0.22909175],\n       [ 2.00540635, -0.56440022,  1.4024378 ,  0.97783065],\n       [-1.15437256,  1.20049949, -1.31755848, -1.43601416],\n       [-0.26963447, -0.34378776, -0.04436873,  0.17321572],\n       [-1.02798141, -1.66746254, -0.21798552, -0.22909175],\n       [ 0.74149478,  0.31804964,  0.47648162,  0.44142069],\n       [-1.53354603,  0.09743717, -1.25968622, -1.30191167],\n       [ 0.36232132, -1.00562515,  1.11307649,  0.30731821],\n       [-0.52241678,  1.86233689, -1.1439417 , -1.03370669],\n       [ 1.37345057,  0.09743717,  0.70797066,  0.44142069],\n       [ 0.61510363, -0.78501269,  0.70797066,  0.84372816],\n       [-1.9127195 , -0.12317529, -1.49117526, -1.43601416],\n       [-1.15437256, -1.44685008, -0.21798552, -0.22909175],\n       [ 1.37345057,  0.31804964,  1.17094875,  1.51424061],\n       [ 2.38457982, -1.00562515,  1.86541588,  1.51424061],\n       [-1.02798141,  0.75927457, -1.25968622, -1.30191167],\n       [ 1.49984172,  0.31804964,  0.59222614,  0.30731821],\n       [ 1.75262404,  1.20049949,  1.4024378 ,  1.78244559],\n       [-1.65993719, -1.66746254, -1.37543074, -1.16780918],\n       [ 0.9942771 , -0.34378776,  0.53435388,  0.17321572],\n       [-1.28076372,  0.09743717, -1.20181396, -1.30191167],\n       [-1.28076372,  0.75927457, -1.20181396, -1.30191167],\n       [-0.14324331, -1.00562515, -0.10224099, -0.22909175],\n       [-0.14324331, -0.56440022,  0.47648162,  0.17321572],\n       [-1.02798141,  0.5386621 , -1.31755848, -1.30191167],\n       [ 1.12066825, -0.12317529,  0.88158744,  1.51424061],\n       [-0.90159025,  1.64172442, -1.25968622, -1.16780918],\n       [-0.52241678, -0.12317529,  0.47648162,  0.44142069],\n       [ 0.74149478, -0.78501269,  0.93945971,  0.97783065],\n       [ 0.61510363, -0.56440022,  0.82371518,  0.44142069],\n       [-0.14324331, -0.12317529,  0.30286483,  0.03911323],\n       [-0.01685215, -0.78501269,  0.12924805,  0.03911323],\n       [-0.52241678,  0.75927457, -1.25968622, -1.03370669],\n       [ 1.24705941, -0.12317529,  1.05520423,  1.24603563],\n       [-1.15437256,  0.09743717, -1.25968622, -1.30191167],\n       [-0.26963447, -0.12317529,  0.24499257,  0.17321572],\n       [-0.14324331, -1.22623762,  0.76584292,  1.11193314],\n       [ 1.62623288, -0.12317529,  1.28669327,  1.24603563],\n       [-0.90159025,  0.75927457, -1.25968622, -1.30191167],\n       [ 0.74149478,  0.31804964,  0.93945971,  1.51424061],\n       [-0.14324331,  1.64172442, -1.1439417 , -1.16780918],\n       [-0.90159025,  1.64172442, -1.20181396, -1.30191167],\n       [-0.01685215, -1.00562515,  0.18712031,  0.03911323],\n       [-1.78632834, -0.12317529, -1.37543074, -1.30191167],\n       [-0.14324331, -0.34378776,  0.30286483,  0.17321572],\n       [ 1.75262404,  0.31804964,  1.34456553,  0.84372816],\n       [ 0.86788594,  0.31804964,  0.82371518,  1.11193314],\n       [-1.53354603,  0.75927457, -1.31755848, -1.16780918],\n       [ 0.48871247, -0.34378776,  0.36073709,  0.17321572],\n       [ 2.38457982, -0.56440022,  1.74967136,  1.11193314],\n       [ 0.36232132, -0.56440022,  0.18712031,  0.17321572],\n       [ 0.36232132, -0.56440022,  0.59222614,  0.03911323],\n       [-0.01685215, -0.78501269,  0.82371518,  0.97783065],\n       [ 1.75262404, -0.12317529,  1.22882101,  0.57552318],\n       [-0.77519909,  2.30356182, -1.25968622, -1.43601416],\n       [-0.90159025,  0.97988703, -1.31755848, -1.30191167],\n       [ 0.61510363, -1.22623762,  0.70797066,  0.44142069],\n       [-0.01685215, -0.78501269,  0.82371518,  0.97783065]])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2tr=sc2.fit_transform(x2tr)\n",
    "x2ts=sc2.transform(x2ts)\n",
    "x2tr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.52241678,  0.75927457, -1.1439417 , -1.30191167],\n       [ 0.61510363,  0.5386621 ,  1.34456553,  1.78244559],\n       [ 0.86788594, -0.12317529,  1.05520423,  0.84372816],\n       [ 0.23593016, -1.88807501,  0.76584292,  0.44142069],\n       [ 0.36232132, -0.12317529,  0.70797066,  0.84372816],\n       [-1.78632834, -0.34378776, -1.31755848, -1.30191167],\n       [ 2.63736213,  1.64172442,  1.57605458,  1.11193314],\n       [ 2.25818866, -0.12317529,  1.6917991 ,  1.24603563],\n       [ 0.61510363,  0.5386621 ,  0.59222614,  0.57552318],\n       [ 1.24705941, -0.56440022,  0.6500984 ,  0.30731821],\n       [ 1.87901519, -0.34378776,  1.51818232,  0.84372816],\n       [ 1.24705941,  0.31804964,  1.28669327,  1.51424061],\n       [-1.28076372, -0.12317529, -1.31755848, -1.43601416],\n       [ 0.48871247,  0.75927457,  0.99733197,  1.51424061],\n       [-1.28076372,  0.75927457, -1.02819717, -1.30191167],\n       [-0.52241678,  1.86233689, -1.37543074, -1.03370669],\n       [-1.40715488,  0.31804964, -1.37543074, -1.30191167],\n       [ 0.36232132, -0.34378776,  0.59222614,  0.30731821],\n       [ 0.74149478, -0.56440022,  1.11307649,  1.38013812],\n       [ 0.86788594, -0.12317529,  1.22882101,  1.38013812],\n       [ 0.36232132, -0.12317529,  0.53435388,  0.30731821],\n       [ 2.38457982, -0.12317529,  1.4024378 ,  1.51424061],\n       [-0.39602562, -1.00562515,  0.41860936,  0.03911323],\n       [-1.53354603,  0.31804964, -1.31755848, -1.30191167],\n       [-1.40715488,  0.31804964, -1.20181396, -1.30191167],\n       [-0.26963447, -0.56440022,  0.70797066,  1.11193314],\n       [-0.14324331, -0.56440022,  0.24499257,  0.17321572],\n       [ 0.74149478, -0.34378776,  0.36073709,  0.17321572],\n       [-0.77519909,  0.97988703, -1.25968622, -1.30191167],\n       [-0.90159025, -1.22623762, -0.3916023 , -0.09498926]])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2ts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "NN_multy2.add(Dense(15,activation='sigmoid',input_dim=4))\n",
    "NN_multy2.add(Dense(15,activation='relu'))\n",
    "NN_multy2.add(Dense(3,activation='relu'))\n",
    "NN_multy2.add(Dense(1,activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 1ms/step - loss: 1.3359 - accuracy: 0.3500\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.2016 - accuracy: 0.3500\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1093 - accuracy: 0.3500\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 1.0270 - accuracy: 0.3500\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9479 - accuracy: 0.3500\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8905 - accuracy: 0.3500\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8367 - accuracy: 0.3500\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7945 - accuracy: 0.3500\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7637 - accuracy: 0.3500\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.7364 - accuracy: 0.3500\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7106 - accuracy: 0.3500\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.3500\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.3500\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.3500\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.6480 - accuracy: 0.3500\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.3500\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6425 - accuracy: 0.3500\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6401 - accuracy: 0.3500\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.3500\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6347 - accuracy: 0.3500\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6317 - accuracy: 0.3500\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6290 - accuracy: 0.3500\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.3500\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6236 - accuracy: 0.3500\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6212 - accuracy: 0.3500\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.3500\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.6156 - accuracy: 0.3500\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.6127 - accuracy: 0.3500\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6102 - accuracy: 0.3500\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.3500\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.6046 - accuracy: 0.3500\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.6020 - accuracy: 0.3500\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.3500\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.3500\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.3500\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5916 - accuracy: 0.3500\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.3500\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5864 - accuracy: 0.3500\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.5838 - accuracy: 0.3500\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.3500\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.3500\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5762 - accuracy: 0.3500\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.3500\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.3500\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.3500\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.3500\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 929us/step - loss: 0.5633 - accuracy: 0.3500\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.3500\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.3500\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5562 - accuracy: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x266a69430a0>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_multy2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "NN_multy2.fit(x2tr,y2tr,batch_size=15,epochs=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_multy2.predict(x2tr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\pycharmprojects\\pythonproject5\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "MLPClassifier(hidden_layer_sizes=100, max_iter=50)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=50)</pre></div></div></div></div></div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(hidden_layer_sizes=100,max_iter=50)\n",
    "mlp.fit(x2tr,y2tr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(x2ts,y2ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\pycharmprojects\\pythonproject5\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2=MLPClassifier(hidden_layer_sizes=100,max_iter=50)\n",
    "mlp2.fit(x_tr_ir,y_tr_ir)\n",
    "mlp2.score(x_ts_ir,y_ts_ir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2.score(x_tr_ir,y_tr_ir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "nn_mlp=Sequential()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "nn_mlp.add(Dense(4,activation='relu',input_dim=4,input_shape=(4,)))\n",
    "nn_mlp.add(Dense(16,activation='relu'))\n",
    "nn_mlp.add(Dense(3,activation='softmax'))\n",
    "#nn_mlp.add(Dense(1,activation='softmax'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "8/8 [==============================] - 2s 2ms/step - loss: 0.4649 - accuracy: 0.6917\n",
      "Epoch 2/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4632 - accuracy: 0.6917\n",
      "Epoch 3/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.6917\n",
      "Epoch 4/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4613 - accuracy: 0.6917\n",
      "Epoch 5/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4603 - accuracy: 0.6917\n",
      "Epoch 6/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4601 - accuracy: 0.6917\n",
      "Epoch 7/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.6917\n",
      "Epoch 8/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4587 - accuracy: 0.6917\n",
      "Epoch 9/60\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4583 - accuracy: 0.6917\n",
      "Epoch 10/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.6917\n",
      "Epoch 11/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.6917\n",
      "Epoch 12/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4571 - accuracy: 0.6917\n",
      "Epoch 13/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.6917\n",
      "Epoch 14/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4563 - accuracy: 0.6917\n",
      "Epoch 15/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.6917\n",
      "Epoch 16/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.6917\n",
      "Epoch 17/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4554 - accuracy: 0.6917\n",
      "Epoch 18/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.6917\n",
      "Epoch 19/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.6917\n",
      "Epoch 20/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4558 - accuracy: 0.6917\n",
      "Epoch 21/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.6917\n",
      "Epoch 22/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.6917\n",
      "Epoch 23/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.6917\n",
      "Epoch 24/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4532 - accuracy: 0.6917\n",
      "Epoch 25/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4531 - accuracy: 0.6917\n",
      "Epoch 26/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4531 - accuracy: 0.6917\n",
      "Epoch 27/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.6917\n",
      "Epoch 28/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4511 - accuracy: 0.6917\n",
      "Epoch 29/60\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.6917\n",
      "Epoch 30/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4498 - accuracy: 0.6917\n",
      "Epoch 31/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.6917\n",
      "Epoch 32/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.6917\n",
      "Epoch 33/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.6917\n",
      "Epoch 34/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.6917\n",
      "Epoch 35/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.6917\n",
      "Epoch 36/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.6917\n",
      "Epoch 37/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.6917\n",
      "Epoch 38/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.6917\n",
      "Epoch 39/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.6917\n",
      "Epoch 40/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.6917\n",
      "Epoch 41/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.6917\n",
      "Epoch 42/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4193 - accuracy: 0.7167\n",
      "Epoch 43/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.6917\n",
      "Epoch 44/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.7333\n",
      "Epoch 45/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4030 - accuracy: 0.8250\n",
      "Epoch 46/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8333\n",
      "Epoch 47/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8500\n",
      "Epoch 48/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3807 - accuracy: 0.8333\n",
      "Epoch 49/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8333\n",
      "Epoch 50/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8667\n",
      "Epoch 51/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8667\n",
      "Epoch 52/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8667\n",
      "Epoch 53/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8667\n",
      "Epoch 54/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8667\n",
      "Epoch 55/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8667\n",
      "Epoch 56/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8667\n",
      "Epoch 57/60\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3017 - accuracy: 0.8667\n",
      "Epoch 58/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8833\n",
      "Epoch 59/60\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2874 - accuracy: 0.8833\n",
      "Epoch 60/60\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2801 - accuracy: 0.8833\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x266b114be50>"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_mlp.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "nn_mlp.fit(x2tr,y2tr,batch_size=15,epochs=60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91381824e-01, 8.61806888e-03, 7.08279160e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91377711e-01, 8.62210616e-03, 7.08922272e-08],\n       [3.85694066e-03, 8.86783838e-01, 1.09359242e-01],\n       [9.91381347e-01, 8.61858577e-03, 7.08361227e-08],\n       [9.91369307e-01, 8.63064639e-03, 7.10285946e-08],\n       [2.64166360e-04, 4.65439975e-01, 5.34295857e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91367042e-01, 8.63284897e-03, 7.10636598e-08],\n       [4.47743246e-03, 8.99475455e-01, 9.60471258e-02],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383135e-01, 8.61672871e-03, 7.08065286e-08],\n       [9.91383433e-01, 8.61645490e-03, 7.08020949e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383255e-01, 8.61670822e-03, 7.08061307e-08],\n       [9.91382539e-01, 8.61740950e-03, 7.08174284e-08],\n       [9.78099299e-04, 7.06220984e-01, 2.92800933e-01],\n       [2.38048378e-02, 9.57527459e-01, 1.86676476e-02],\n       [9.91381228e-01, 8.61860998e-03, 7.08365135e-08],\n       [3.00667286e-02, 9.55376506e-01, 1.45566687e-02],\n       [1.49301020e-04, 3.70216638e-01, 6.29634023e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [6.52822247e-03, 9.25164104e-01, 6.83077127e-02],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91381705e-01, 8.61818995e-03, 7.08297918e-08],\n       [8.19468405e-03, 9.36541319e-01, 5.52639887e-02],\n       [4.26180399e-04, 5.53518116e-01, 4.46055651e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.94855258e-02, 9.57499504e-01, 2.30149571e-02],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.43702822e-02, 9.53932226e-01, 3.16975378e-02],\n       [1.10536935e-02, 9.47813451e-01, 4.11327742e-02],\n       [9.91383135e-01, 8.61681066e-03, 7.08077508e-08],\n       [4.86766832e-04, 5.79154909e-01, 4.20358360e-01],\n       [9.91381705e-01, 8.61818250e-03, 7.08297918e-08],\n       [9.91376996e-01, 8.62291828e-03, 7.09051520e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91379261e-01, 8.62069707e-03, 7.08698948e-08],\n       [4.58343141e-02, 9.45034027e-01, 9.13164672e-03],\n       [6.06905483e-02, 9.32803035e-01, 6.50647702e-03],\n       [9.91383314e-01, 8.61655269e-03, 7.08037078e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383553e-01, 8.61625466e-03, 7.07989969e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383553e-01, 8.61625466e-03, 7.07991390e-08],\n       [4.00503259e-03, 8.90705645e-01, 1.05289340e-01],\n       [2.78307125e-04, 4.74733382e-01, 5.24988353e-01],\n       [9.91369963e-01, 8.62986222e-03, 7.10161743e-08],\n       [9.91382658e-01, 8.61724094e-03, 7.08147425e-08],\n       [9.91383314e-01, 8.61649495e-03, 7.08027628e-08],\n       [9.91383255e-01, 8.61670822e-03, 7.08061307e-08],\n       [8.27594325e-02, 9.12459791e-01, 4.78076003e-03],\n       [6.67347312e-02, 9.27227199e-01, 6.03798265e-03],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383255e-01, 8.61660577e-03, 7.08045107e-08],\n       [7.81801045e-02, 9.16784942e-01, 5.03492635e-03],\n       [5.07635511e-02, 9.41065729e-01, 8.17071088e-03],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91378069e-01, 8.62184167e-03, 7.08881913e-08],\n       [5.17042354e-04, 5.89845777e-01, 4.09637183e-01],\n       [9.91383314e-01, 8.61654896e-03, 7.08037078e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91378427e-01, 8.62142909e-03, 7.08814625e-08],\n       [8.68792757e-02, 9.08554077e-01, 4.56656236e-03],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91382062e-01, 8.61784816e-03, 7.08244201e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.88734901e-01, 1.12649454e-02, 1.18751679e-07],\n       [1.81351876e-04, 4.01218623e-01, 5.98599970e-01],\n       [9.91376340e-01, 8.62354971e-03, 7.09152488e-08],\n       [9.91382301e-01, 8.61756690e-03, 7.08198442e-08],\n       [4.05679569e-02, 9.48990285e-01, 1.04417205e-02],\n       [2.18916498e-03, 8.26161802e-01, 1.71649054e-01],\n       [9.91380870e-01, 8.61896761e-03, 7.08423045e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383433e-01, 8.61645490e-03, 7.08020949e-08],\n       [2.62288842e-02, 9.57112968e-01, 1.66580845e-02],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.53865913e-04, 3.74911726e-01, 6.24934375e-01],\n       [1.13669835e-01, 8.82981062e-01, 3.34917987e-03],\n       [8.28064792e-03, 9.37180161e-01, 5.45392446e-02],\n       [9.91379738e-01, 8.62014666e-03, 7.08610060e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91375744e-01, 8.62413272e-03, 7.09245427e-08],\n       [1.18041530e-01, 8.78752112e-01, 3.20640579e-03],\n       [1.58906507e-04, 3.79793346e-01, 6.20047688e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91381824e-01, 8.61803535e-03, 7.08273760e-08],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91382897e-01, 8.61702580e-03, 7.08113745e-08],\n       [9.91383433e-01, 8.61642603e-03, 7.08016898e-08],\n       [1.13458475e-02, 9.48587000e-01, 4.00672406e-02],\n       [9.91376340e-01, 8.62359162e-03, 7.09160588e-08],\n       [5.58875734e-03, 9.15810823e-01, 7.86003768e-02],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91382539e-01, 8.61739647e-03, 7.08171584e-08],\n       [4.98856883e-04, 5.83054006e-01, 4.16447133e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.27029000e-03, 7.49432266e-01, 2.49297395e-01],\n       [1.12108537e-03, 7.29704618e-01, 2.69174218e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01],\n       [9.91383553e-01, 8.61625466e-03, 7.07989969e-08],\n       [9.91382658e-01, 8.61722138e-03, 7.08143304e-08],\n       [5.75123937e-04, 6.10043824e-01, 3.89381111e-01],\n       [1.30375760e-04, 3.49581838e-01, 6.50287747e-01]], dtype=float32)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr=nn_mlp.predict(x2tr)\n",
    "pr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "(120, 3)"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.3037576e-04, 3.4958184e-01, 6.5028775e-01], dtype=float32)"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[0,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def label(predict):\n",
    "    labels_=[]\n",
    "    for i in range(predict.shape[0]):\n",
    "        row=predict[i,:]\n",
    "        max=np.max(row)\n",
    "        for j in range(len(row)):\n",
    "            if predict[i,j]==max:\n",
    "                labels_.append(j)\n",
    "    return np.array(labels_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2, 2, 0, 2, 0, 1, 0, 0, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1,\n       1, 0, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 2, 1, 2, 1, 1, 0, 1, 0, 0, 2,\n       0, 1, 1, 0, 2, 0, 2, 2, 2, 2, 0, 1, 2, 0, 0, 0, 0, 1, 1, 2, 0, 1,\n       1, 2, 0, 1, 0, 2, 2, 0, 1, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2,\n       0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 1, 2, 2, 0,\n       1, 2, 1, 1, 2, 2, 0, 0, 1, 2])"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb=label(pr)\n",
    "lb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2,\n       1, 0, 1, 1, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 2,\n       0, 1, 1, 0, 2, 0, 1, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1,\n       1, 1, 0, 2, 0, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 2,\n       0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 1, 2, 2, 0,\n       1, 2, 1, 1, 2, 2, 0, 0, 1, 2])"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2tr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8916666666666667"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=lb==y2tr\n",
    "np.sum(mask)/len(y2tr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000266AB317F40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.3885 - accuracy: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.3885360360145569, 0.8666666746139526]"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_mlp.evaluate(x2ts,y2ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}